{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# VLM Comparison: Base vs Fine-tuned\n\nCompare your fine-tuned VLM model against the base model to evaluate improvements.\n\n## Usage\n1. Set `EXP_DIR` to your experiment directory (e.g., `runs/exp_20250102_xxx`)\n2. Run all cells - paths are auto-loaded from experiment config\n\n## Notes\n- Config is auto-loaded from `{EXP_DIR}/config.json` (saved during training)\n- For end-to-end testing (YOLO + VLM), use `predict.py --vlm`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom pathlib import Path\n\n# ============================================================\n# ONLY SET THIS - everything else is auto-loaded from config\n# ============================================================\nEXP_DIR = \"../runs/exp_xxx\"  # Your experiment directory\n# ============================================================\n\n# Load experiment config\nconfig_path = Path(EXP_DIR) / \"config.json\"\nif not config_path.exists():\n    raise FileNotFoundError(\n        f\"Config not found: {config_path}\\n\"\n        \"Make sure you're using an experiment trained with the latest train.py\"\n    )\n\nwith open(config_path) as f:\n    config = json.load(f)\n\n# Auto-load settings from config\nMODEL_NAME = config[\"vlm\"][\"model\"]\nPRECISION = config[\"vlm\"][\"precision\"]\nVLM_DATA_DIR = config[\"vlm\"][\"data_dir\"]\nADAPTER_PATH = config[\"vlm\"][\"adapter\"] or f\"{EXP_DIR}/vlm/best\"\n\nprint(f\"Loaded config from: {config_path}\")\nprint(f\"  Model: {MODEL_NAME}\")\nprint(f\"  Precision: {PRECISION}\")\nprint(f\"  VLM Data: {VLM_DATA_DIR}\")\nprint(f\"  Adapter: {ADAPTER_PATH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test images - VLM validation set (auto-loaded from config)\nTEST_FOLDER = f\"{VLM_DATA_DIR}/images/val\"\n\n# Your evaluation prompt - customize if needed\nPROMPT = \"\"\"\nLook at the bounding box in the image.\nWhat do you see inside the marked area?\n\"\"\".strip()\n\n# Optional: System prompt (should match training config)\nSYSTEM_PROMPT = \"\"\"\nYou are an object detection assistant.\nWhen shown an image with a bounding box, identify what is inside the marked area.\n\"\"\".strip()\n\n# Set to None if you didn't use system prompt during training\n# SYSTEM_PROMPT = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Find test images (limited for quick comparison)\nimport glob\nimport random\n\nMAX_IMAGES = 20  # Limit for comparison - adjust as needed\n\nimage_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.webp\"]\nall_images = []\n\nfor ext in image_extensions:\n    all_images.extend(glob.glob(f\"{TEST_FOLDER}/{ext}\"))\n    all_images.extend(glob.glob(f\"{TEST_FOLDER}/**/{ext}\", recursive=True))\n\nall_images = sorted(set(all_images))\n\n# Filter out global images (_all.jpg) - they have multiple boxes\n# Only keep per-box images (_box*.jpg) for \"marked area\" prompts\nper_box_images = [img for img in all_images if \"_all.\" not in img]\nglobal_images = [img for img in all_images if \"_all.\" in img]\n\nprint(f\"Found {len(all_images)} total images:\")\nprint(f\"  - {len(per_box_images)} per-box images (single bbox)\")\nprint(f\"  - {len(global_images)} global images (multiple bboxes) - excluded\")\n\n# Sample from per-box images only\nif len(per_box_images) > MAX_IMAGES:\n    test_images = random.sample(per_box_images, MAX_IMAGES)\n    test_images = sorted(test_images)\n    print(f\"\\nSampled {MAX_IMAGES} per-box images for comparison:\")\nelse:\n    test_images = per_box_images\n    print(f\"\\nUsing {len(test_images)} per-box images:\")\n\nfor img in test_images[:10]:\n    print(f\"  - {img.split('/')[-1]}\")\nif len(test_images) > 10:\n    print(f\"  ... and {len(test_images) - 10} more\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '..')\n\nfrom yologen.models.vlm.qwen import QwenVLM\n\nprint(\"Loading base model...\")\nbase_vlm = QwenVLM(\n    model_name=MODEL_NAME,\n    load_in_4bit=(PRECISION == \"4bit\"),\n    load_in_8bit=(PRECISION == \"8bit\"),\n    use_lora=False,\n)\nbase_vlm.load_model()\nprint(\"Base model ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Base Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results = []\n",
    "\n",
    "for i, image_path in enumerate(test_images):\n",
    "    print(f\"\\r[Base] Processing {i+1}/{len(test_images)}: {image_path.split('/')[-1]}\", end=\"\")\n",
    "    \n",
    "    response = base_vlm.generate(\n",
    "        image=image_path,\n",
    "        question=PROMPT,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "    )\n",
    "    \n",
    "    base_results.append({\n",
    "        \"image\": image_path,\n",
    "        \"response\": response.strip()\n",
    "    })\n",
    "\n",
    "print(f\"\\nBase model: {len(base_results)} images processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clear GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "del base_vlm\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"GPU memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading fine-tuned model...\")\n",
    "finetuned_vlm = QwenVLM(\n",
    "    model_name=MODEL_NAME,\n",
    "    load_in_4bit=(PRECISION == \"4bit\"),\n",
    "    load_in_8bit=(PRECISION == \"8bit\"),\n",
    "    use_lora=False,\n",
    ")\n",
    "finetuned_vlm.load_model()\n",
    "finetuned_vlm.load_adapter(ADAPTER_PATH)\n",
    "print(f\"Fine-tuned model ready! Adapter: {ADAPTER_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Fine-tuned Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_results = []\n",
    "\n",
    "for i, image_path in enumerate(test_images):\n",
    "    print(f\"\\r[Fine-tuned] Processing {i+1}/{len(test_images)}: {image_path.split('/')[-1]}\", end=\"\")\n",
    "    \n",
    "    response = finetuned_vlm.generate(\n",
    "        image=image_path,\n",
    "        question=PROMPT,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "    )\n",
    "    \n",
    "    finetuned_results.append({\n",
    "        \"image\": image_path,\n",
    "        \"response\": response.strip()\n",
    "    })\n",
    "\n",
    "print(f\"\\nFine-tuned model: {len(finetuned_results)} images processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Visual Comparison"
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nfrom PIL import Image\nimport textwrap\n\nn_images = min(len(test_images), 12)\n\nfig, axes = plt.subplots(n_images, 2, figsize=(24, 6*n_images),\n                         gridspec_kw={'width_ratios': [1.5, 1]})\n\nif n_images == 1:\n    axes = axes.reshape(1, -1)\n\nfor i in range(n_images):\n    base = base_results[i]\n    ft = finetuned_results[i]\n    \n    # Left: Image (larger)\n    try:\n        img = Image.open(base[\"image\"])\n        axes[i, 0].imshow(img)\n        axes[i, 0].set_title(base[\"image\"].split(\"/\")[-1], fontsize=11, fontweight='bold')\n        axes[i, 0].axis('off')\n    except Exception as e:\n        axes[i, 0].text(0.5, 0.5, f\"Error: {e}\", ha='center', va='center')\n        axes[i, 0].axis('off')\n    \n    # Right: Responses\n    axes[i, 1].axis('off')\n    \n    # Wrap long text\n    base_text = textwrap.fill(base[\"response\"], width=45)\n    ft_text = textwrap.fill(ft[\"response\"], width=45)\n    \n    # Base response (gray background)\n    axes[i, 1].text(0.02, 0.78, \"BASE MODEL:\", fontsize=12, fontweight='bold',\n                    color='#333', transform=axes[i, 1].transAxes, va='top')\n    axes[i, 1].text(0.02, 0.68, base_text, fontsize=11,\n                    color='#333', transform=axes[i, 1].transAxes, va='top',\n                    bbox=dict(boxstyle='round', facecolor='#f0f0f0', edgecolor='#ccc', pad=0.5))\n    \n    # Fine-tuned response (blue background)\n    axes[i, 1].text(0.02, 0.38, \"FINE-TUNED:\", fontsize=12, fontweight='bold',\n                    color='#0066cc', transform=axes[i, 1].transAxes, va='top')\n    axes[i, 1].text(0.02, 0.28, ft_text, fontsize=11,\n                    color='#0066cc', transform=axes[i, 1].transAxes, va='top',\n                    bbox=dict(boxstyle='round', facecolor='#e6f2ff', edgecolor='#99ccff', pad=0.5))\n\nplt.tight_layout()\nsave_path = f\"{EXP_DIR}/comparison_results.png\"\nplt.savefig(save_path, dpi=150, bbox_inches='tight')\nplt.show()\nprint(f\"\\nCompared {n_images} images. Saved to: {save_path}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del finetuned_vlm\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}