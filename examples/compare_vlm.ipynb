{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM Comparison: Base vs Fine-tuned\n",
    "\n",
    "Compare your fine-tuned VLM model against the base model to evaluate improvements.\n",
    "\n",
    "## Usage\n",
    "1. Set your model name and adapter path\n",
    "2. Define your test prompt\n",
    "3. Point to your test images folder\n",
    "4. Run all cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-VL-7B-Instruct\"  # Base model name\n",
    "ADAPTER_PATH = \"/path/to/your/vlm/adapter\"   # Fine-tuned adapter path\n",
    "PRECISION = \"4bit\"                           # 4bit, 8bit, or fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test images folder\n",
    "TEST_FOLDER = \"test_images\"  # Folder containing test images (with bbox drawn)\n",
    "\n",
    "# Your evaluation prompt - customize for your use case\n",
    "# The images should have red bounding boxes already drawn\n",
    "PROMPT = \"\"\"\n",
    "Look at the red bounding box in the image.\n",
    "Is there a [YOUR_OBJECT] inside the marked area?\n",
    "\n",
    "Answer with only: Yes or No\n",
    "\"\"\".strip()\n",
    "\n",
    "# Optional: System prompt (set to None if not used during training)\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an object detection assistant.\n",
    "When shown an image with a red bounding box, identify what is inside the marked area.\n",
    "\"\"\".strip()\n",
    "\n",
    "# Set to None if you didn't use system prompt during training\n",
    "# SYSTEM_PROMPT = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all test images\n",
    "import glob\n",
    "\n",
    "image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.webp\"]\n",
    "test_images = []\n",
    "\n",
    "for ext in image_extensions:\n",
    "    test_images.extend(glob.glob(f\"{TEST_FOLDER}/{ext}\"))\n",
    "    test_images.extend(glob.glob(f\"{TEST_FOLDER}/**/{ext}\", recursive=True))\n",
    "\n",
    "test_images = sorted(set(test_images))\n",
    "\n",
    "print(f\"Found {len(test_images)} test images:\")\n",
    "for img in test_images[:10]:  # Show first 10\n",
    "    print(f\"  - {img}\")\n",
    "if len(test_images) > 10:\n",
    "    print(f\"  ... and {len(test_images) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yologen.models.vlm.qwen import QwenVLM\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "base_vlm = QwenVLM(\n",
    "    model_name=MODEL_NAME,\n",
    "    load_in_4bit=(PRECISION == \"4bit\"),\n",
    "    load_in_8bit=(PRECISION == \"8bit\"),\n",
    "    use_lora=False,\n",
    ")\n",
    "base_vlm.load_model()\n",
    "print(\"Base model ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Base Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results = []\n",
    "\n",
    "for i, image_path in enumerate(test_images):\n",
    "    print(f\"\\r[Base] Processing {i+1}/{len(test_images)}: {image_path.split('/')[-1]}\", end=\"\")\n",
    "    \n",
    "    response = base_vlm.generate(\n",
    "        image=image_path,\n",
    "        question=PROMPT,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "    )\n",
    "    \n",
    "    base_results.append({\n",
    "        \"image\": image_path,\n",
    "        \"response\": response.strip()\n",
    "    })\n",
    "\n",
    "print(f\"\\nBase model: {len(base_results)} images processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clear GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "del base_vlm\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"GPU memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading fine-tuned model...\")\n",
    "finetuned_vlm = QwenVLM(\n",
    "    model_name=MODEL_NAME,\n",
    "    load_in_4bit=(PRECISION == \"4bit\"),\n",
    "    load_in_8bit=(PRECISION == \"8bit\"),\n",
    "    use_lora=False,\n",
    ")\n",
    "finetuned_vlm.load_model()\n",
    "finetuned_vlm.load_adapter(ADAPTER_PATH)\n",
    "print(f\"Fine-tuned model ready! Adapter: {ADAPTER_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Fine-tuned Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_results = []\n",
    "\n",
    "for i, image_path in enumerate(test_images):\n",
    "    print(f\"\\r[Fine-tuned] Processing {i+1}/{len(test_images)}: {image_path.split('/')[-1]}\", end=\"\")\n",
    "    \n",
    "    response = finetuned_vlm.generate(\n",
    "        image=image_path,\n",
    "        question=PROMPT,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "    )\n",
    "    \n",
    "    finetuned_results.append({\n",
    "        \"image\": image_path,\n",
    "        \"response\": response.strip()\n",
    "    })\n",
    "\n",
    "print(f\"\\nFine-tuned model: {len(finetuned_results)} images processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count Yes/No responses\n",
    "def count_responses(results):\n",
    "    counts = Counter()\n",
    "    for r in results:\n",
    "        resp = r[\"response\"].lower()\n",
    "        if \"yes\" in resp:\n",
    "            counts[\"Yes\"] += 1\n",
    "        elif \"no\" in resp:\n",
    "            counts[\"No\"] += 1\n",
    "        else:\n",
    "            counts[\"Other\"] += 1\n",
    "    return counts\n",
    "\n",
    "base_counts = count_responses(base_results)\n",
    "ft_counts = count_responses(finetuned_results)\n",
    "\n",
    "# Agreement rate\n",
    "agreements = sum(1 for b, f in zip(base_results, finetuned_results) \n",
    "                 if b[\"response\"].lower().strip() == f[\"response\"].lower().strip())\n",
    "agreement_rate = agreements / len(test_images) * 100\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nTotal images: {len(test_images)}\")\n",
    "print(f\"\\nBase Model:       Yes={base_counts['Yes']}, No={base_counts['No']}, Other={base_counts['Other']}\")\n",
    "print(f\"Fine-tuned Model: Yes={ft_counts['Yes']}, No={ft_counts['No']}, Other={ft_counts['Other']}\")\n",
    "print(f\"\\nAgreement rate: {agreement_rate:.1f}% ({agreements}/{len(test_images)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "html = \"<table style='width:100%; border-collapse: collapse; font-size:12px;'>\"\n",
    "html += \"<tr style='background:#333; color:white;'>\"\n",
    "html += \"<th style='padding:8px; border:1px solid #ddd;'>#</th>\"\n",
    "html += \"<th style='padding:8px; border:1px solid #ddd;'>Image</th>\"\n",
    "html += \"<th style='padding:8px; border:1px solid #ddd;'>Base Model</th>\"\n",
    "html += \"<th style='padding:8px; border:1px solid #ddd;'>Fine-tuned</th>\"\n",
    "html += \"<th style='padding:8px; border:1px solid #ddd;'>Match</th>\"\n",
    "html += \"</tr>\"\n",
    "\n",
    "for i, (base, ft) in enumerate(zip(base_results, finetuned_results)):\n",
    "    base_resp = base[\"response\"][:30]\n",
    "    ft_resp = ft[\"response\"][:30]\n",
    "    match = base[\"response\"].lower().strip() == ft[\"response\"].lower().strip()\n",
    "    match_icon = \"✓\" if match else \"✗\"\n",
    "    match_color = \"#90EE90\" if match else \"#FFB6C1\"\n",
    "    \n",
    "    html += f\"<tr>\"\n",
    "    html += f\"<td style='padding:6px; border:1px solid #ddd;'>{i+1}</td>\"\n",
    "    html += f\"<td style='padding:6px; border:1px solid #ddd;'>{base['image'].split('/')[-1]}</td>\"\n",
    "    html += f\"<td style='padding:6px; border:1px solid #ddd;'>{base_resp}</td>\"\n",
    "    html += f\"<td style='padding:6px; border:1px solid #ddd;'>{ft_resp}</td>\"\n",
    "    html += f\"<td style='padding:6px; border:1px solid #ddd; background:{match_color}; text-align:center;'>{match_icon}</td>\"\n",
    "    html += f\"</tr>\"\n",
    "\n",
    "html += \"</table>\"\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "# Grid layout\n",
    "n_images = min(len(test_images), 12)  # Show max 12 images\n",
    "n_cols = 3\n",
    "n_rows = math.ceil(n_images / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 5*n_rows))\n",
    "axes = axes.flatten() if n_images > 1 else [axes]\n",
    "\n",
    "for i in range(n_images):\n",
    "    base = base_results[i]\n",
    "    ft = finetuned_results[i]\n",
    "    \n",
    "    try:\n",
    "        img = Image.open(base[\"image\"])\n",
    "        axes[i].imshow(img)\n",
    "        \n",
    "        # Color: Yes=green, No=red, Other=gray\n",
    "        def get_color(resp):\n",
    "            if \"yes\" in resp.lower(): return \"green\"\n",
    "            if \"no\" in resp.lower(): return \"red\"\n",
    "            return \"gray\"\n",
    "        \n",
    "        base_color = get_color(base[\"response\"])\n",
    "        ft_color = get_color(ft[\"response\"])\n",
    "        \n",
    "        # Labels on image\n",
    "        axes[i].text(0.02, 0.98, f\"Base: {base['response'][:15]}\", \n",
    "                     transform=axes[i].transAxes, fontsize=9, fontweight='bold',\n",
    "                     color='white', backgroundcolor=base_color,\n",
    "                     verticalalignment='top')\n",
    "        axes[i].text(0.02, 0.88, f\"FT: {ft['response'][:15]}\", \n",
    "                     transform=axes[i].transAxes, fontsize=9, fontweight='bold',\n",
    "                     color='white', backgroundcolor=ft_color,\n",
    "                     verticalalignment='top')\n",
    "        \n",
    "        axes[i].set_title(base[\"image\"].split(\"/\")[-1], fontsize=8)\n",
    "        axes[i].axis('off')\n",
    "    except Exception as e:\n",
    "        axes[i].text(0.5, 0.5, f\"Error: {e}\", ha='center', va='center')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "# Hide empty subplots\n",
    "for j in range(n_images, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"comparison_results.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\nSaved to: comparison_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "output_file = \"comparison_results.csv\"\n",
    "\n",
    "with open(output_file, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Image\", \"Base_Response\", \"Finetuned_Response\", \"Match\"])\n",
    "    \n",
    "    for base, ft in zip(base_results, finetuned_results):\n",
    "        match = base[\"response\"].lower().strip() == ft[\"response\"].lower().strip()\n",
    "        writer.writerow([\n",
    "            base[\"image\"],\n",
    "            base[\"response\"],\n",
    "            ft[\"response\"],\n",
    "            \"Yes\" if match else \"No\"\n",
    "        ])\n",
    "\n",
    "print(f\"Results exported to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del finetuned_vlm\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
