{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# VLM Comparison: Base vs Fine-tuned\n\nCompare your fine-tuned VLM model against the base model to evaluate improvements.\n\n## Usage\n1. Set `EXP_DIR` to your experiment directory (e.g., `runs/exp_20250102_xxx`)\n2. Run all cells - paths are auto-loaded from experiment config\n\n## Notes\n- Config is auto-loaded from `{EXP_DIR}/config.json` (saved during training)\n- For end-to-end testing (YOLO + VLM), use `predict.py --vlm`"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom pathlib import Path\n\n# ============================================================\n# ONLY SET THIS - everything else is auto-loaded from config\n# ============================================================\nEXP_DIR = \"../runs/exp_xxx\"  # Your experiment directory\n# ============================================================\n\n# Load experiment config\nconfig_path = Path(EXP_DIR) / \"config.json\"\nif not config_path.exists():\n    raise FileNotFoundError(\n        f\"Config not found: {config_path}\\n\"\n        \"Make sure you're using an experiment trained with the latest train.py\"\n    )\n\nwith open(config_path) as f:\n    config = json.load(f)\n\n# Auto-load settings from config\nMODEL_NAME = config[\"vlm\"][\"model\"]\nPRECISION = config[\"vlm\"][\"precision\"]\nVLM_DATA_DIR = config[\"vlm\"][\"data_dir\"]\nADAPTER_PATH = config[\"vlm\"][\"adapter\"] or f\"{EXP_DIR}/vlm/best\"\n\nprint(f\"Loaded config from: {config_path}\")\nprint(f\"  Model: {MODEL_NAME}\")\nprint(f\"  Precision: {PRECISION}\")\nprint(f\"  VLM Data: {VLM_DATA_DIR}\")\nprint(f\"  Adapter: {ADAPTER_PATH}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test images - VLM validation set (auto-loaded from config)\nTEST_FOLDER = f\"{VLM_DATA_DIR}/images/val\"\n\n# Your evaluation prompt - customize if needed\nPROMPT = \"\"\"\nLook at the bounding box in the image.\nWhat do you see inside the marked area?\n\"\"\".strip()\n\n# Optional: System prompt (should match training config)\nSYSTEM_PROMPT = \"\"\"\nYou are an object detection assistant.\nWhen shown an image with a bounding box, identify what is inside the marked area.\n\"\"\".strip()\n\n# Set to None if you didn't use system prompt during training\n# SYSTEM_PROMPT = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all test images\n",
    "import glob\n",
    "\n",
    "image_extensions = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.webp\"]\n",
    "test_images = []\n",
    "\n",
    "for ext in image_extensions:\n",
    "    test_images.extend(glob.glob(f\"{TEST_FOLDER}/{ext}\"))\n",
    "    test_images.extend(glob.glob(f\"{TEST_FOLDER}/**/{ext}\", recursive=True))\n",
    "\n",
    "test_images = sorted(set(test_images))\n",
    "\n",
    "print(f\"Found {len(test_images)} test images:\")\n",
    "for img in test_images[:10]:  # Show first 10\n",
    "    print(f\"  - {img}\")\n",
    "if len(test_images) > 10:\n",
    "    print(f\"  ... and {len(test_images) - 10} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '..')\n\nfrom yologen.models.vlm.qwen import QwenVLM\n\nprint(\"Loading base model...\")\nbase_vlm = QwenVLM(\n    model_name=MODEL_NAME,\n    load_in_4bit=(PRECISION == \"4bit\"),\n    load_in_8bit=(PRECISION == \"8bit\"),\n    use_lora=False,\n)\nbase_vlm.load_model()\nprint(\"Base model ready!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Base Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results = []\n",
    "\n",
    "for i, image_path in enumerate(test_images):\n",
    "    print(f\"\\r[Base] Processing {i+1}/{len(test_images)}: {image_path.split('/')[-1]}\", end=\"\")\n",
    "    \n",
    "    response = base_vlm.generate(\n",
    "        image=image_path,\n",
    "        question=PROMPT,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "    )\n",
    "    \n",
    "    base_results.append({\n",
    "        \"image\": image_path,\n",
    "        \"response\": response.strip()\n",
    "    })\n",
    "\n",
    "print(f\"\\nBase model: {len(base_results)} images processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clear GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "del base_vlm\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"GPU memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading fine-tuned model...\")\n",
    "finetuned_vlm = QwenVLM(\n",
    "    model_name=MODEL_NAME,\n",
    "    load_in_4bit=(PRECISION == \"4bit\"),\n",
    "    load_in_8bit=(PRECISION == \"8bit\"),\n",
    "    use_lora=False,\n",
    ")\n",
    "finetuned_vlm.load_model()\n",
    "finetuned_vlm.load_adapter(ADAPTER_PATH)\n",
    "print(f\"Fine-tuned model ready! Adapter: {ADAPTER_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run Fine-tuned Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_results = []\n",
    "\n",
    "for i, image_path in enumerate(test_images):\n",
    "    print(f\"\\r[Fine-tuned] Processing {i+1}/{len(test_images)}: {image_path.split('/')[-1]}\", end=\"\")\n",
    "    \n",
    "    response = finetuned_vlm.generate(\n",
    "        image=image_path,\n",
    "        question=PROMPT,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "    )\n",
    "    \n",
    "    finetuned_results.append({\n",
    "        \"image\": image_path,\n",
    "        \"response\": response.strip()\n",
    "    })\n",
    "\n",
    "print(f\"\\nFine-tuned model: {len(finetuned_results)} images processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Comparison Table",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from IPython.display import display, HTML\n\nhtml = \"<table style='width:100%; border-collapse: collapse; font-size:12px;'>\"\nhtml += \"<tr style='background:#333; color:white;'>\"\nhtml += \"<th style='padding:8px; border:1px solid #ddd;'>#</th>\"\nhtml += \"<th style='padding:8px; border:1px solid #ddd;'>Image</th>\"\nhtml += \"<th style='padding:8px; border:1px solid #ddd;'>Base Model</th>\"\nhtml += \"<th style='padding:8px; border:1px solid #ddd;'>Fine-tuned</th>\"\nhtml += \"</tr>\"\n\nfor i, (base, ft) in enumerate(zip(base_results, finetuned_results)):\n    base_resp = base[\"response\"][:50]\n    ft_resp = ft[\"response\"][:50]\n    \n    html += f\"<tr>\"\n    html += f\"<td style='padding:6px; border:1px solid #ddd;'>{i+1}</td>\"\n    html += f\"<td style='padding:6px; border:1px solid #ddd;'>{base['image'].split('/')[-1]}</td>\"\n    html += f\"<td style='padding:6px; border:1px solid #ddd;'>{base_resp}</td>\"\n    html += f\"<td style='padding:6px; border:1px solid #ddd;'>{ft_resp}</td>\"\n    html += f\"</tr>\"\n\nhtml += \"</table>\"\ndisplay(HTML(html))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Visual Comparison"
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nfrom PIL import Image\nimport math\n\n# Grid layout\nn_images = min(len(test_images), 12)  # Show max 12 images\nn_cols = 3\nn_rows = math.ceil(n_images / n_cols)\n\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 5*n_rows))\naxes = axes.flatten() if n_images > 1 else [axes]\n\nfor i in range(n_images):\n    base = base_results[i]\n    ft = finetuned_results[i]\n    \n    try:\n        img = Image.open(base[\"image\"])\n        axes[i].imshow(img)\n        \n        # Labels on image\n        axes[i].text(0.02, 0.98, f\"Base: {base['response'][:20]}\", \n                     transform=axes[i].transAxes, fontsize=9, fontweight='bold',\n                     color='white', backgroundcolor='#333333',\n                     verticalalignment='top')\n        axes[i].text(0.02, 0.88, f\"FT: {ft['response'][:20]}\", \n                     transform=axes[i].transAxes, fontsize=9, fontweight='bold',\n                     color='white', backgroundcolor='#0066cc',\n                     verticalalignment='top')\n        \n        axes[i].set_title(base[\"image\"].split(\"/\")[-1], fontsize=8)\n        axes[i].axis('off')\n    except Exception as e:\n        axes[i].text(0.5, 0.5, f\"Error: {e}\", ha='center', va='center')\n        axes[i].axis('off')\n\n# Hide empty subplots\nfor j in range(n_images, len(axes)):\n    axes[j].axis('off')\n\nplt.tight_layout()\nplt.savefig(\"comparison_results.png\", dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"\\nSaved to: comparison_results.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del finetuned_vlm\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}