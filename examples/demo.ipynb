{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# YoloGen Demo\n",
    "\n",
    "**YOLO Object Detection + VLM Description Pipeline**\n",
    "\n",
    "This notebook demonstrates how to use trained models for inference and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. YOLO Detection Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "from yologen.core.predictor import YOLOPredictor\n\n# Load YOLO model\n# Replace with your trained model path\nyolo = YOLOPredictor(weights=\"../runs/your_experiment/yolo/weights/best.pt\")\n\n# Run prediction\nresults = yolo.predict(\"path/to/your/image.jpg\")\n\n# Display results\nfor det in results[0]['detections']:\n    print(f\"Class: {det['class_name']}, Conf: {det['confidence']:.2f}, BBox: {det['bbox']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. YOLO + VLM (Unified Predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "from yologen.core.predictor import UnifiedPredictor\n\n# Initialize unified predictor (YOLO + VLM)\n# Replace with your trained model paths\npredictor = UnifiedPredictor(\n    yolo_weights=\"../runs/your_experiment/yolo/weights/best.pt\",\n    vlm_adapter=\"../runs/your_experiment/vlm/best\",\n    vlm_precision=\"4bit\",\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# Run prediction with VLM description\nimg_path = \"path/to/your/image.jpg\"  # Replace with your image path\n\nresults = predictor.predict(\n    source=img_path,\n    vlm_question=\"What is in the red marked area?\",\n)\n\n# Display results as text\nfor det in results[0]['detections']:\n    print(f\"\\n[{det['class_name']}] conf={det['confidence']:.2f}\")\n    print(f\"  BBox: {det['bbox']}\")\n    print(f\"  VLM: {det['vlm_answer']}\")\n\n# Quick visualization\nimg = cv2.imread(img_path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\nfor det in results[0]['detections']:\n    x1, y1, x2, y2 = [int(v) for v in det['bbox']]\n    cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 3)\n    cv2.putText(img, det['class_name'], (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n\nplt.figure(figsize=(12, 8))\nplt.imshow(img)\nplt.axis('off')\n\n# Show VLM answer below image\nvlm_answer = results[0]['detections'][0]['vlm_answer'] if results[0]['detections'] else \"No detection\"\nplt.figtext(0.5, 0.02, f\"VLM: {vlm_answer}\", ha='center', fontsize=11, \n            wrap=True, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\nplt.title(\"YOLO + VLM Prediction\", fontsize=12)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Visualize Predictions\n",
    "\n",
    "Display the image with detection boxes and VLM descriptions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_with_vlm(image_path, detections, box_color=(255, 0, 0), box_thickness=3):\n",
    "    \"\"\"\n",
    "    Visualize detections with bounding boxes and VLM descriptions.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image file\n",
    "        detections: List of detection results with vlm_answer\n",
    "        box_color: RGB tuple for box color (default: red)\n",
    "        box_thickness: Line thickness for boxes\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    img = cv2.imread(str(image_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Draw boxes and labels\n",
    "    for i, det in enumerate(detections):\n",
    "        x1, y1, x2, y2 = [int(v) for v in det['bbox']]\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), box_color, box_thickness)\n",
    "        \n",
    "        # Draw label with index\n",
    "        label = f\"[{i+1}] {det['class_name']}: {det['confidence']:.2f}\"\n",
    "        cv2.putText(img, label, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, box_color, 2)\n",
    "    \n",
    "    # Create figure with image on top, VLM results below\n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Image subplot (top 70%)\n",
    "    ax_img = fig.add_axes([0.05, 0.30, 0.9, 0.65])\n",
    "    ax_img.imshow(img)\n",
    "    ax_img.axis('off')\n",
    "    ax_img.set_title('YOLO Detection + VLM Description', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # VLM results text (bottom 25%)\n",
    "    ax_text = fig.add_axes([0.05, 0.02, 0.9, 0.25])\n",
    "    ax_text.axis('off')\n",
    "    \n",
    "    # Build VLM results text\n",
    "    vlm_text = \"VLM Descriptions:\\n\" + \"=\" * 50 + \"\\n\\n\"\n",
    "    for i, det in enumerate(detections):\n",
    "        vlm_text += f\"[{i+1}] {det['class_name']} (conf: {det['confidence']:.2f})\\n\"\n",
    "        vlm_text += f\"    â†’ {det['vlm_answer']}\\n\\n\"\n",
    "    \n",
    "    ax_text.text(0, 1, vlm_text, transform=ax_text.transAxes, fontsize=11,\n",
    "                 verticalalignment='top', fontfamily='monospace',\n",
    "                 bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the prediction results\n",
    "fig = visualize_with_vlm(\n",
    "    image_path=img_path,\n",
    "    detections=results[0]['detections'],\n",
    "    box_color=(255, 0, 0),  # Red (RGB)\n",
    "    box_thickness=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Load Settings from Training Config\n",
    "\n",
    "For consistent results, load box color and thickness from the training config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "def load_vlm_config(vlm_adapter_path):\n    \"\"\"\n    Load VLM settings from training config.json.\n    \n    Returns dict with: box_thickness, box_color (RGB), system_prompt, class_names\n    \"\"\"\n    adapter_path = Path(vlm_adapter_path)\n    \n    # Search for config.json in adapter directory or parent\n    search_paths = [\n        adapter_path / \"config.json\",\n        adapter_path.parent / \"config.json\",\n    ]\n    \n    for config_path in search_paths:\n        if config_path.exists():\n            with open(config_path) as f:\n                config = json.load(f)\n            return {\n                'box_thickness': config.get('box_thickness', 3),\n                'box_color': tuple(config.get('box_color', [255, 0, 0])),  # RGB\n                'system_prompt': config.get('system_prompt', None),\n                'class_names': config.get('class_names', []),\n            }\n    \n    # Default values if config not found\n    return {'box_thickness': 3, 'box_color': (255, 0, 0), 'system_prompt': None, 'class_names': []}\n\n\n# Load config from training\n# Replace with your VLM adapter path\nvlm_config = load_vlm_config(\"../runs/your_experiment/vlm/best\")\n\nprint(\"Loaded VLM Config:\")\nprint(f\"  box_thickness: {vlm_config['box_thickness']}\")\nprint(f\"  box_color: {vlm_config['box_color']} (RGB)\")\nprint(f\"  system_prompt: {'Yes' if vlm_config['system_prompt'] else 'No'}\")\nprint(f\"  class_names: {vlm_config['class_names']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# OPTIONAL: Override settings if needed\n",
    "# ============================================================\n",
    "# Uncomment and modify to override training config:\n",
    "#\n",
    "# vlm_config['box_color'] = (0, 255, 0)    # Green\n",
    "# vlm_config['box_color'] = (0, 0, 255)    # Blue  \n",
    "# vlm_config['box_thickness'] = 5          # Thicker lines\n",
    "\n",
    "# Visualize with loaded/overridden config\n",
    "fig = visualize_with_vlm(\n",
    "    image_path=img_path,\n",
    "    detections=results[0]['detections'],\n",
    "    box_color=vlm_config['box_color'],\n",
    "    box_thickness=vlm_config['box_thickness'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Custom Questions\n",
    "\n",
    "Ask different questions about the detected objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example questions to ask the VLM\n",
    "questions = [\n",
    "    \"What is in the red marked area?\",\n",
    "    \"Describe the object in the red box.\",\n",
    "    \"What color is the vehicle?\",\n",
    "    \"Is this a truck or a car?\",\n",
    "]\n",
    "\n",
    "print(\"Asking different questions about the same detection:\\n\")\n",
    "for q in questions:\n",
    "    results = predictor.predict(\n",
    "        source=img_path,\n",
    "        vlm_question=q,\n",
    "    )\n",
    "    answer = results[0]['detections'][0]['vlm_answer'] if results[0]['detections'] else \"No detection\"\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Batch Processing\n",
    "\n",
    "Process multiple images from a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "# Process multiple images from a directory\n# Replace with your image directory\nimage_dir = Path(\"../data/your_dataset/images/val\")\nimages = list(image_dir.glob(\"*.jpg\"))[:5]  # First 5 images\n\nprint(f\"Processing {len(images)} images...\\n\")\n\nfor img_path in images:\n    results = predictor.predict(\n        source=str(img_path),\n        vlm_question=\"What is in the red marked area?\",\n    )\n    \n    print(f\"{img_path.name}:\")\n    for det in results[0]['detections']:\n        vlm_short = det['vlm_answer'][:60] + \"...\" if len(det['vlm_answer']) > 60 else det['vlm_answer']\n        print(f\"  [{det['class_name']}] {vlm_short}\")\n    print()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. Save Results\n",
    "\n",
    "Save predictions with visualizations and VLM descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results with --save flag\n",
    "# This creates both annotated images and .txt files with VLM answers\n",
    "\n",
    "results = predictor.predict(\n",
    "    source=img_path,\n",
    "    vlm_question=\"What is in the red marked area?\",\n",
    "    save=True,\n",
    "    save_dir=\"./output\",\n",
    ")\n",
    "\n",
    "print(\"Results saved to ./output/\")\n",
    "print(\"  - *_result.jpg: Annotated image\")\n",
    "print(\"  - *_result.txt: VLM descriptions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": "## 9. Training Your Own Model\n\n```bash\n# Train YOLO + VLM with single command\npython train.py --config configs/car_detection.yaml\n\n# Train YOLO only\npython train.py --data data/your_dataset/dataset.yaml --epochs 100\n\n# Skip YOLO, train VLM only (if YOLO already trained)\npython train.py --config configs/car_detection.yaml --skip-yolo\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}