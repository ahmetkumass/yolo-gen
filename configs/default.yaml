# YoloGen Default Configuration
# Works on most hardware (8GB+ VRAM)
#
# Usage: python train.py --config configs/default.yaml
#
# For custom datasets, copy this file and modify:
#   1. data: path to your dataset.yaml
#   2. vlm_dataset.prompts: your Q&A templates
#   3. vlm_dataset.details: class-specific descriptions

# ============================================================
# DATASET
# ============================================================
data: data/your_dataset/dataset.yaml

# ============================================================
# YOLO TRAINING
# ============================================================
yolo:
  model: yolov8n.pt               # nano (fastest), s/m/l/x for accuracy
  epochs: 100
  batch: 16                       # Reduce if OOM
  imgsz: 640

  # Optimizer
  optimizer: auto
  lr0: 0.01
  lrf: 0.01
  momentum: 0.937
  weight_decay: 0.0005
  warmup_epochs: 3

  # Loss weights
  box: 7.5
  cls: 0.5
  dfl: 1.5

  # Augmentation
  hsv_h: 0.015
  hsv_s: 0.7
  hsv_v: 0.4
  degrees: 0.0
  translate: 0.1
  scale: 0.5
  fliplr: 0.5
  mosaic: 1.0
  mixup: 0.0

  # Early stopping
  patience: 50

# ============================================================
# VLM TRAINING (Vision-Language Model)
# ============================================================
# GPU Memory (4-bit QLoRA, ~1M max_pixels):
#   Qwen 3 VL: 2B=~14GB, 4B=~18GB, 8B=~24GB
#   Qwen 2.5 VL: 3B=~22GB, 7B=~27GB
#   Inference (4-bit): ~8-12GB
vlm:
  enabled: true
  # Model options:
  #   Qwen 2.5 VL (patch_size=28):
  #     Qwen/Qwen2.5-VL-3B-Instruct  - 3B params
  #     Qwen/Qwen2.5-VL-7B-Instruct  - 7B params
  #
  #   Qwen 3 VL (patch_size=32, recommended):
  #     Qwen/Qwen3-VL-2B-Instruct    - 2B params, fastest
  #     Qwen/Qwen3-VL-2B-Thinking    - 2B params, reasoning
  #     Qwen/Qwen3-VL-4B-Instruct    - 4B params, balanced
  #     Qwen/Qwen3-VL-4B-Thinking    - 4B params, reasoning
  #     Qwen/Qwen3-VL-8B-Instruct    - 8B params, best quality
  #     Qwen/Qwen3-VL-8B-Thinking    - 8B params, reasoning + quality
  model: Qwen/Qwen3-VL-4B-Instruct

  # Training
  epochs: 3
  batch_size: 1                   # Keep at 1 (higher = 2x memory per step)

  # Image size - MAJOR impact on GPU memory
  # Aspect ratio is preserved automatically
  # Values are auto-calculated based on model version if not specified:
  #   - Qwen 2.5: patch_size=28, default max_pixels = 1280*28*28 = 1003520
  #   - Qwen 3:   patch_size=32, default max_pixels = 1280*32*32 = 1310720
  #
  # Override presets (pick based on your GPU and use case):
  #   - Small objects/details: max_pixels: 2073600  (1920x1080, ~50-60GB)
  #   - High quality:          max_pixels: 1503520  (1280x1280, ~35-40GB)
  #   - Default (balanced):    max_pixels: ~1M      (~1000x1000, ~22-27GB)
  #   - Low memory:            max_pixels: 512000   (700x700, ~15-18GB)
  # min_pixels: null              # Auto-calculated (256 * patch_size^2)
  # max_pixels: null              # Auto-calculated (1280 * patch_size^2)
  lr: 0.00002
  precision: 4bit                 # 4bit required for <80GB VRAM
  max_samples: 10000              # Limit samples (null = use all)

  # LoRA (Low-Rank Adaptation)
  lora_r: 64                      # 32/64/128 - minimal memory impact
  lora_alpha: 16
  lora_dropout: 0.05

  # Memory optimization
  gradient_checkpointing: true    # Required - saves ~20-30GB
  gradient_accumulation: 4        # No memory impact, effective_batch = batch * this

# ============================================================
# VLM DATASET GENERATION
# ============================================================
vlm_dataset:
  box_color: [0, 0, 255]          # BGR Red (OpenCV format)
  box_thickness: 3

  system_prompt: |
    You are an object detection assistant.
    When shown an image with a red bounding box, identify and describe
    the object inside the marked area clearly and concisely.

  # Prompt templates - customize for your use case
  # Placeholders: {class}, {detail}, {yes_no}, {explanation}
  prompts:
    - question: "What is in the red marked area?"
      answer: "The red marked area contains a {class}. {detail}"

    - question: "Is there an object in the red bounding box?"
      answer: "{yes_no}, {explanation}."

    - question: "Describe the object inside the red rectangle."
      answer: "Inside the red rectangle, I can see a {class}. {detail}"

    - question: "What can you identify in the highlighted area?"
      answer: "The highlighted area shows a {class}."

  # Class-specific descriptions (customize per class)
  # details:
  #   person:
  #     - "A person is visible in the frame."
  #   car:
  #     - "A vehicle/car is detected."

# ============================================================
# OUTPUT
# ============================================================
output:
  save_dir: runs
  export_onnx: true
